-- see: https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d
\X.\Y.
{ sigmoid ← [1+ℯ(_x)];
  sigmoidDdx ← [x*(1-x)];
  dot ← [(+)/1 0 ((*)`x y)];
  vmul ← λA.λx. (dot x)`{1∘[2]} A;
  forward ← λwh.λwo.λbh.λbo.
    { ho ← sigmoid'1 ((+)` bh (vmul X wh))
    ; prediction ← sigmoid (bo + (dot ho wo))
    ; (ho,prediction)
    };
  train ← λwh.λwo.λbh.λbo.
    { o ← forward wh wo bh bo
    ; ho ⟜ o->1; prediction ⟜ o->2
    ; l1Err ← (-prediction)'1Y
    ; l1Delta ← (*(sigmoidDdx prediction))'1 l1Err
    ; hiddenErr ← l1Delta (*)⊗ wo
    -- outer product??
    ; hiddenDelta ← [(*)`(sigmoidDdx'1 ho) x]'1 hiddenErr 
    ; hiddenDelta
    -- (wha,woa,bha,boa)
    };
  train
}
