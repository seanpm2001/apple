-- see: https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d
{ X ⟜ ⟨⟨0,0⟩,⟨0,1⟩,⟨1,0⟩,⟨1,1⟩⟩;
  Y ⟜ ⟨0,1,1,0⟩;
  sigmoid ← [1%(1+ℯ(_x))];
  sDdx ← [x*(1-x)];
  sum ⇐ [(+)/x];
  forward ← λwh.λwo.λbh.λbo.
    -- ho: 4x2
    { ho ← sigmoid`{0} ([(+)`bh x]'(X%.wh))
    -- prediction: 4
    ; prediction ← sigmoid'((+bo)'(ho%:wo))
    ; (ho,prediction)
    };
  -- wh: 2x2 wo: 2 bh: 2 bo: (scalar)
  train ← λinp.
    { wh ← inp->1; wo ← inp->2; bh ← inp->3; bo ← inp->4
    ; o ← forward wh wo bh bo
    ; ho ⟜ o->1; prediction ⟜ o->2
    ; l1E ← (-)`prediction Y
    ; l1Δ ← (*)`(sDdx'prediction) l1E -- 4
    ; he ← l1Δ (*)⊗ wo -- 4x2
    ; hΔ ← (*)`{0,0} (sDdx`{0} ho) he -- 4x2
    ; wha ← (+)`{0,0} wh ((|:X)%.hΔ)
    ; woa ← (+)`wo ((|:ho)%:l1Δ)
    ; bha ← [(+)/ₒ x y]`{0,1} bh hΔ
    ; boa ← bo + sum l1Δ
    ; (wha,woa,bha,boa)
    };
  wh ⟜ 𝔯_1 1;wo ⟜ 𝔯_1 1;bh ⟜ 𝔯_1 1;bo ⟜ 𝔯_1 1;
  train^:10000 (wh,wo,bh,bo)
}
