-- see: https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d
-- wh: 2x2 wo: 2 bh: 2 bo: (scalar)
λwh.λwo.λbh.λbo.
{ X ⟜ ⟨⟨0,0⟩,⟨0,1⟩,⟨1,0⟩,⟨1,1⟩⟩;
  Y ⟜ ⟨0,1,1,0⟩;
  sigmoid ← [1%(1+ℯ(_x))];
  sDdx ← [x*(1-x)];
  sum ⇐ [(+)/x];
  -- ho: 4x2
  -- prediction: 4
  ho ← sigmoid`{0} ([(+)`bh x]'(X%.wh));
  prediction ← sigmoid'((+bo)'(ho%:wo));
  l1E ← (-)`Y prediction;
  l1Δ ← (*)`(sDdx'prediction) l1E; -- 4
  he ← l1Δ (*)⊗ wo; -- 4x2
  hΔ ⟜ (*)`{0,0} (sDdx`{0} ho) he; -- 4x2
  wha ← (+)`{0,0} wh ((|:X)%.hΔ);
  woa ← (|:ho)%:l1Δ;
  bha ← sum'((<|)`{0,1} bh hΔ);
  boa ← bo + sum l1Δ;
  hΔ -- (wha,woa,bha,boa)
}
-- train ⟨⟨0.51426693,0.56885825⟩,⟨0.48725347,0.15041493⟩⟩ ⟨0.14801747,0.37182892⟩ ⟨0.79726405,0.67601843⟩ 0.57823076
